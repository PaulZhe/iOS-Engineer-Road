[TOC]

# 图像显示原理

![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8p8ksn0ljj30sa0b7q36.jpg)

关于CPU和GPU都是由总线连接起来的，在CPU输出的往往是一个位图，上传给GPU。GPU拿到位图之后，会做一个图层的渲染，包括纹理的合成，最后会把结果放到帧缓冲区中， 由视图控制器根据`VSync信号`（帧同步信号,表示扫描1帧的开始，一帧也就是LCD显示的一个画面。）在指定时间之前，在帧缓冲区当中提取对应显示内容，最后展示到屏幕显示器上。

## CPU和GPU分别干了哪些事

![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8p8cz38v2j30ts0fkt9q.jpg)

`UIView`的显示部分是由`CALayer`来负责的，`CALayer`里有一个`contents`属性，就是我们最终要绘制到屏幕上面的一个位图，比如要绘制一个`UILabel`，`contents`里就放的是里面文字的位图，然后系统就会在合适的时机回调给我们一个`drawRect`方法，然后我们可以在此基础之上，在上面绘制一些我们自己想绘制的内容。

绘制好的位图，会经由`Core Animation`这个框架，提交给`GPU`部分的`OpenGL`渲染管线，进行最后位图的渲染包括纹理的合成，然后会显示到屏幕上。

虚线左侧发生在CPU，虚线右侧在GPU。

## CPU和GPU在具体的工作上都有哪些承担

1. 首先对于CPU来说，要完成UI的布局，包括显示或者说绘制，之后会做一些准备工作，最后会将位图提交到GPU上面。

   ![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8z3xiwlrhj30zl03bmx7.jpg)

- Layout：
  - UI布局
  - 文本计算

- Display：
  - 绘制（`drawRect`）

- Prepare：
  - 图片编解码
- Commit：
  - 提交位图

2. GPU渲染管线的过程：实际上这个过程指的就是`OpenGL`的渲染管线

   ![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8z4b2b7aej314k0joq3n.jpg)

   关于GPU渲染管线这五个步骤做完之后，就会把最终的像素点，提交到对应的帧缓冲区中。

# UI卡顿和掉帧的原因

![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8z50yx787j31400eu3z4.jpg)

我们一般说页面滑动的流畅性是60fps，指的就是每一秒钟会有60帧的画面更新。相当于每隔16.7ms就要产生一帧的数据，那么在这16.6ms之内呢，就要CPU和GPU共同协作完成产生最终一帧的数据。

比如说CPU花费一帧的时间做文本的布局，UI计算包括一些视图的绘制、视频解码，最终把产生的位图提交给GPU，再由GPU进行图层的合成、纹理渲染，准备好下一帧的画面，在下一帧的`VSync`信号到来的时候就会显示这一帧的画面。

假如CPU在做刚才一系列工作的耗时过长的话，留给GPU的时间就非常少，等GPU把它所需的工作准备完毕，这个时长可能就会超过16.7ms。这种情况下在下一帧`VSync`信号到来的时候，这一帧的视图还没有准备好，由此产生了掉帧，我们看到的效果就是视图滑动时的卡顿，这就是UI卡顿、掉帧的原因。

总结成一句话，就是在规定的16.7ms之内，在下一帧`VSync`信号到来之前，并没有CPU和GPU共同完成下一帧画面的合成，这就会造成卡顿或者说掉帧。

# 滑动优化方案前的准备知识

## FPS

- FPS是图像领域中的定义，是指画面每秒传输帧数，通俗来讲就是指动画或视频的画面数。FPS是测量用于保存、显示动态视频的信息数量。每秒钟帧数越多，所显示的动作就会越流畅。通常，要避免动作不流畅的最低是30。
- iPhone的FPS为60，也就是说它在一秒钟内会刷新60次，每次间隔大概16.7ms，也就是说我们的iPhone每次有16.7ms的时间来处理事件

## 像素点如何出现到屏幕上

首先来看一张软件组成图：

![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8zp21xvvkj30rj0a7weo.jpg)

- 这张图从从右到左就是软件到硬件，从App到硬件屏幕上出现界面的全过程
- 再来逐个解析一下这几个模块

### 渲染参与者

- GPU Driver：GPU驱动软件，直接和 GPU 交流的代码块

- OpenGL：提供了 2D 和 3D 图形渲染的 API，高GPU的能力，并实现硬件加速渲染，是第一个和图形硬件(GPU)交流的标准化方式

- Core Graphics：Quartz 2D的一个高级绘图引擎，常用与iOS，tvOS，macOS的图形绘制应用开发。Core Graphics是对底层C语言的一个简单封装，其中提供大量的低层次，轻量级的2D渲染API。【前缀为CG，比如常见的CGPath，CGColor】

- Core Animation：是苹果提供的一套基于绘图的动画框架，但不止是动画，他同样是绘图的根本【前缀是CA，比如CALayer】

  ![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8zp73o91zj30gh0c73yt.jpg)

- 从图中可以看出，最底层是图形硬件(GPU)；上层是OpenGL和CoreGraphics，提供一些接口来访问GPU；再上层的CoreAnimation在此基础上封装了一套动画的API。最上面的UIKit属于应用层，处理与用户的交互。

- Core Image：iOS处理图像的框架，主要用处可以给图片添加滤镜效果和图像识别功能。

## 像素相关知识

- 屏幕上的像素是由红，绿，蓝三种颜色组件构成的。因此，位图数据有时也被叫做 RGB 数据。

- ARGB：32bits-per-pixel(bpp), 8bits-per-componet(bpc),透明度会首先被乘以到像素值上【也就是说对于透明度的处理我们直接就是百分比乘到RGB值里面】。

  ```
   A   R   G   B   A   R   G   B   A   R   G   B  
  | pixel 0       | pixel 1       | pixel 2   
    0   1   2   3   4   5   6   7   8   9   10  11 ...
    
  这个格式经常被叫做 ARGB。每个像素占用 4 字节(32bpp),每一个颜色组件是1字节(8bpc).每个像素有一个 alpha 值，这个值总是最先得到的(在RGB值之前)，最终红、绿、蓝的值都会被预先乘以 alpha 的值
  
  如果我们有一个橙色，他们各自的 8bpc 就像这样: 240,99,24.一个完全不透明的橙色像素拥有的 ARGB 值为: 255，240，99，24，它在内存中的布局就像上面图示那样。如果我们有一个相同颜色的像素，但是 alpha 值为 33%，那么他的像素值便是:84，80，33，8.
  ```

- xRGB：就是ARGB跳过第一个像素值，即并没有任何 alpha 值(他们都被假定为100%不透明)，但是内存布局是一样的，这样子将会节约内存，且每一个独立的像素都对齐到 32-bit 的边界。

## 合成过程

合成过程：因为在图形世界中，合成是一个描述不同位图如何放到一起来创建你最终在屏幕上看到图形的过程。

假定屏幕上一切事物皆纹理。一个纹理就是一个包含 RGBA 值的长方形，比如，每一个像素里面都包含红、绿、蓝和透明度的值。在 Core Animation 世界中这就相当于一个 CALayer。

在这个简化的设置中，每一个 layer 是一个纹理，所有的纹理都以某种方式堆叠在彼此的顶部。对于屏幕上的每一个像素，GPU 需要算出怎么混合这些纹理来得到像素 RGB 的值。这就是合成大概的意思。

理想状态（像素对齐的情况下）的像素合成公式：

```
R = S + D * ( 1 – Sa )
//结果的颜色 = 源色彩(顶端纹理) + 目标颜色(低一层的纹理) * (1 - 源颜色的透明度)。
//在这个公式中所有的颜色都假定已经预先乘以了他们的透明度。
```

##透明与不透明：

当源纹理是完全不透明的时候，目标像素就等于源纹理。这可以省下 GPU 很大的工作量，这样只需简单的拷贝源纹理而不需要合成所有的像素值。但是没有方法能告诉 GPU 纹理上的像素是透明还是不透明的。这也是为什么 CALayer 有一个叫做 opaque 的属性了。如果这个属性为 YES，GPU 将不会做任何合成，而是简单从这个层拷贝，不需要考虑它下方的任何东西(因为都被它遮挡住了)。这节省了 GPU 相当大的工作量。

如果你加载一个没有 alpha 通道的图片，并且将它显示在 UIImageView 上，会自动设置opaque 为 YES。

## 对齐与不对齐
如果几个图层的模版都是完美重合，那我们只要从第一个像素到最后一个像素都计算合成一下
但是如果像素没有对齐好，我们还需要额外进行额外的移位操作，合并原纹理上的像素

两种情况会导致不对齐出现：

1. 缩放
2. 当纹理的起点不在一个像素的边界上

# 滑动优化方案

- 减轻CPU工作的负担出发
  - 对于对象创建、控件调整、销毁放在子线程去做，节省一部分CPU的时间
  - 预排版（布局计算、文本计算）都放在子线程去做，这样主线程就会有更多的时间相应用户的交互
  - 预渲染（文本等异步绘制，图片编解码等）
- GPU
  - 纹理渲染
    - 规避离频渲染
    - 通过CPU异步绘制的方式减轻GPU的压力
  - 视图混合
    - 比如视图层级十分复杂，那GPU需要合成每一个对应像素点的像素值，做大量的计算，这个合成过程也会变得复杂。减轻视图层级的复杂性，会减轻GPU合成视图时的压力。
    - 包括CPU的异步绘制机制，来达到提交的位图本身就是一个层级非常少的视图，这样也可以减轻GPU的压力。

# UIView的绘制原理

- 首先通过一个流程图来讲解这个大概过程

  ![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8zyspy2nxj31070mbta1.jpg)

  - 调用 UIView 的`setNeedsDisplay`之后并没有立即执行当前视图的绘制工作，而是在调用时立即调用当前view 的 layer 的同名方法，于是在当前 layer 上打上了一个脏标记，在当前 runloop快要结束的时候才会调用`CALayer display`方法，然后才会进行当前视图真正的绘制流程当中。

  - `CALayer display`方法的内部实现当中首先会判断这个 layer 的 delegate 是否响应`displayLayer`方法。

    如果不响应，就会进入系统的绘制流程当中，如果响应这个方法，就会提供异步绘制的入口。

- 系统的绘制流程：

  ![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8zzokpzs6j31310mnq4d.jpg)

  - 在 CALayer 内部它会创建一个`backing store`，可以理解为 `CGContextRef`（上下文），一般在`drawRect`方法中，我们可以通过上下文堆栈中取出栈顶的`context`，拿到的就是当前控件或者说视图的上下文。

  - 然后 layer 会判断它是否有代理，如果没有代理的话会调用`CALayer drawInContext:`方法

    如果有代理，会调用layer代理方法` drawLayer:inContext:`，然后做视图绘制工作，这一部分是发生在系统内部当中的。然后在合适的时机会回调给我们一个方法，就是我们熟悉的`UIView drawRect`，调用`drawRect`方法，默认是什么都不做的，这就给我们开了一个口子，允许我们在系统的绘制之上，做一些其他的绘制工作。

    然后不论是哪个分支，都是由 CALayer 上传 `backing store`到GPU，这里面的 `backing store`就可以理解为系统最后的位图，最后结束系统绘制流程。

# 异步绘制

- 怎样进行异步绘制？

  就是基于系统给我们开的口子，layer.delegate如果遵从（实现）了`displayLayer`方法的话呢，我们就可以进入异步绘制的流程当中。

- 代理（layer.delegate）负责生成对应的 bitmap（位图）。

- 设置该 bitmap 作为 layer.contents 属性的值。

- 异步绘制流程：

  ![](https://tva1.sinaimg.cn/large/006y8mN6ly1g900amr4qnj314t0lsmyn.jpg)

  - 左侧是主队列，右侧是全局并发队列，假如我们在某一时机一个 View 调用了`setNeedsDisplay`这个方法之后呢，在当前 runloop 将要结束的时候呢，系统就会调用视图所对应 layer 的`display`方法。

    如果我们的代理实现了`displayLayer:`这个函数的时候，会调用代理的`displayLayer:`这个方法，然后会通过子线程的切换，在子线程中进行位图的绘制。主线程这会就可以做一些其他的工作。

    子线程在全局并发队列中所做的工作：
    - 通过`CGBitmapContextCreate()`这个 Core Graphics 的函数来创建位图的一个上下文。
    - 再通过 Core Graphics 的相关API做当前UI控件的一些绘制工作。
    - 之后通过 Core Graphics 函数`CGBitmapContextCreateImage()`来根据当前绘制的上下文来生成一张 CGImage 的图片。

    之后再回到主队列当中，提交这个位图，设置给 CALayer 的`contents`属性，这样就完成了一个UI控件 的异步绘制过程。

# 离频渲染

## 什么是离屏渲染？

- 我们先来看看在屏渲染（On-Screen Rendering）的概念：

  指的是**GPU**的渲染操作是在当前用于显示的屏幕缓冲区中进行。

- 再来看看离屏渲染（Off-Screen Rendering）：

  指的是**GPU**在当前屏幕缓冲区以外**新开辟**一个缓冲区进行渲染操作。

- 用通俗的语言总结一下：当我们在设置某些UI视图的图层属性，如果说指令为在未预合成之前，不能用于直接显示的时候呢，那么就触发了离屏渲染。

  典型的就是我们设置视图的圆角属性，还有一些蒙层（Mask）遮罩。

## 何时会触发离屏渲染

- 圆角（当和`maskToBounds`一起使用，即该属性设为YES时才会触发离屏渲染，缺一不可）
- 图层蒙版
- 阴影
- 光栅化

## 为何要避免离屏渲染？

CPU 和 GPU 在做具体的渲染过程中做了大量的工作，而离屏渲染是发生在 GPU 层面上面的，使 GPU 层面上面触发了 OpenGL 多通道渲染管线，产生了额外的开销，所以需要避免离屏渲染。

- 标准回答：**在触发离屏渲染的时候，会增加 GPU 的工作量，而增加 GPU 的工作量很有可能会到导致CPU和GPU工作总耗时超出了16.7ms，那么可能就会导致UI的卡顿和掉帧，那么我们就要避免离屏渲染。**

- 另一种回答：
  - 会创建新的渲染缓冲区，会有内存上的开销
  - 会有上下文的切换，因为有多通道渲染管线，要把多通道的渲染结果进行一个合成，那么就有GPU一个额外的开销。

# 总结

- 什么是离屏渲染？

  离屏渲染的概念起源于GPU，那GPU层面上呢，如果在当前屏幕缓冲区之外新开辟一个缓冲区去进行渲染操作的话呢，那么就是离屏渲染。

- UIView 和 CALayer 之间的关系是怎样的？

  UIView是专门负责事件传递和事件响应的，而CALayer全权负责UI视图的显示工作。这就用到了六大设计原则中的单一职责原则。



**参考文章**：[绘制像素到屏幕上](https://objccn.io/issue-3-1/)